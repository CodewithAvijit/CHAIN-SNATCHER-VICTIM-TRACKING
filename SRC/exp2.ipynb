{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRCgz6Hncb0r"
   },
   "source": [
    "# Data Preparation for pretrained i3d kinetics **400**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WQOBaXTXitA"
   },
   "outputs": [],
   "source": [
    "import os, cv2, numpy as np, random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "# ✅ Final Settings - All Correct\n",
    "input_root = \"/content/drive/MyDrive/Dataset\"\n",
    "output_root = \"/content/drive/MyDrive/clips_dataset_32frames_final\"\n",
    "classes = [\"Snatching\", \"Normal\"]\n",
    "num_frames = 32  # ✅ 32 frames per video\n",
    "val_split = 0.2\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "def simplified_sampling(total_frames, num_samples=32):\n",
    "    return np.linspace(0, max(0, total_frames-1), num_samples, dtype=int).tolist()\n",
    "\n",
    "def apply_augmentation(frames, is_training=True):\n",
    "    \"\"\"Apply augmentation only to training data\"\"\"\n",
    "    if not is_training:\n",
    "        return frames\n",
    "\n",
    "    augmented_frames = []\n",
    "    # Decide augmentation parameters once for whole video\n",
    "    do_flip = random.random() < 0.5\n",
    "    do_brightness = random.random() < 0.3\n",
    "    brightness_factor = random.uniform(0.7, 1.3)\n",
    "    do_contrast = random.random() < 0.25\n",
    "    contrast_factor = random.uniform(0.8, 1.2)\n",
    "    do_saturation = random.random() < 0.2\n",
    "    saturation_factor = random.uniform(0.8, 1.2)\n",
    "    do_rotation = random.random() < 0.15\n",
    "    rotation_angle = random.uniform(-8, 8)\n",
    "\n",
    "    for frame in frames:\n",
    "        pil_frame = Image.fromarray(frame)\n",
    "\n",
    "        if do_flip:\n",
    "            pil_frame = pil_frame.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        if do_brightness:\n",
    "            enhancer = ImageEnhance.Brightness(pil_frame)\n",
    "            pil_frame = enhancer.enhance(brightness_factor)\n",
    "        if do_contrast:\n",
    "            enhancer = ImageEnhance.Contrast(pil_frame)\n",
    "            pil_frame = enhancer.enhance(contrast_factor)\n",
    "        if do_saturation:\n",
    "            enhancer = ImageEnhance.Color(pil_frame)\n",
    "            pil_frame = enhancer.enhance(saturation_factor)\n",
    "        if do_rotation:\n",
    "            pil_frame = pil_frame.rotate(rotation_angle, fillcolor=(128, 128, 128))\n",
    "\n",
    "        augmented_frames.append(np.array(pil_frame))\n",
    "\n",
    "    return augmented_frames\n",
    "\n",
    "# ✅ Main Processing Loop\n",
    "for cls in classes:\n",
    "    in_dir = os.path.join(input_root, cls)\n",
    "    videos = sorted(os.listdir(in_dir))\n",
    "    random.shuffle(videos)\n",
    "    split_idx = int(len(videos)*(1-val_split))\n",
    "    train_videos = videos[:split_idx]\n",
    "    val_videos = videos[split_idx:]\n",
    "\n",
    "    for split, vids in ((\"train\", train_videos), (\"val\", val_videos)):\n",
    "        out_dir = os.path.join(output_root, split, cls)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        print(f\"Processing {len(vids)} videos for {split}/{cls} (32 frames, normalized)\")\n",
    "\n",
    "        for vid in tqdm(vids):\n",
    "            path = os.path.join(in_dir, vid)\n",
    "            cap = cv2.VideoCapture(path)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            indices = simplified_sampling(total_frames, num_frames)\n",
    "\n",
    "            frames = []\n",
    "            i = 0\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                if i in indices:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frame_resized = cv2.resize(frame_rgb, (224,224))  # ✅ 224x224\n",
    "                    frames.append(frame_resized)\n",
    "                i += 1\n",
    "            cap.release()\n",
    "\n",
    "            if len(frames) == 0:\n",
    "                print(\"empty?\", path)\n",
    "                continue\n",
    "\n",
    "            # Padding if needed\n",
    "            while len(frames) < num_frames:\n",
    "                frames.append(frames[-1].copy())\n",
    "\n",
    "            # ✅ Apply augmentation (only for training)\n",
    "            is_training = (split == \"train\")\n",
    "            augmented_frames = apply_augmentation(frames, is_training)\n",
    "\n",
    "            # ✅ Convert to float32 and normalize to [0,1]\n",
    "            clip = np.stack(augmented_frames, axis=0).astype(np.float32)  # (32, 224, 224, 3)\n",
    "            clip = clip / 255.0  # Convert to [0,1]\n",
    "\n",
    "            # ✅ Apply ImageNet normalization\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            clip = (clip - mean) / std\n",
    "\n",
    "            # ✅ Convert to I3D format: (3, 32, 224, 224)\n",
    "            clip = np.transpose(clip, (3, 0, 1, 2)).astype(np.float32)\n",
    "\n",
    "            name = os.path.splitext(vid)[0]\n",
    "            np.save(os.path.join(out_dir, f\"{name}.npy\"), clip)\n",
    "\n",
    "print(\"✅ FINAL PIPELINE COMPLETE!\")\n",
    "print(\"Ready for I3D training with perfect format!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJGDOAYBcoh3"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "Training **Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7syyEttcspI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Avijit/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1013 training samples and 127 validation samples.\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# === 1. Imports ===\n",
    "import os, random, time\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    ")\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === 2. Dataset (MODIFIED for .pt files and flexible class mapping) ===\n",
    "class PtClipDataset(Dataset):\n",
    "    # Mapping the folder names (keys) to the model class index (values)\n",
    "    # Based on your folder structure (person, snatcher, victim) and your 2 target classes (Snatching, Normal)\n",
    "    CLASS_MAP = {\n",
    "        \"person\": 1,     # Assuming 'person' folder contains 'Normal' clips (index 1)\n",
    "        \"snatcher\": 0,   # Assuming 'snatcher' folder contains 'Snatching' clips (index 0)\n",
    "        \"victim\": 0      # Assuming 'victim' folder contains 'Snatching' clips (index 0)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, root, split=\"TRAINDATA\"): # split default changed to match your folder\n",
    "        self.samples = []\n",
    "        \n",
    "        # Get all relevant class folders inside the split directory\n",
    "        split_path = os.path.join(root, split)\n",
    "        if not os.path.isdir(split_path):\n",
    "            print(f\"Error: Split directory not found at {split_path}\")\n",
    "            return\n",
    "\n",
    "        # Iterate only over the folders we've defined a mapping for\n",
    "        for folder_name, class_index in self.CLASS_MAP.items():\n",
    "            folder = os.path.join(split_path, folder_name)\n",
    "            if not os.path.isdir(folder):\n",
    "                print(f\"Warning: Class folder not found at {folder}\")\n",
    "                continue\n",
    "            \n",
    "            # 🔥 Change 1: Search for \"*.pt\" files instead of \"*.npy\"\n",
    "            for f in glob(os.path.join(folder, \"*.pt\")):\n",
    "                self.samples.append((f, class_index))\n",
    "        \n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        \n",
    "        # 🔥 Change 2: Use torch.load() instead of np.load()\n",
    "        # Load the PyTorch tensor clip\n",
    "        # Using map_location='cpu' to avoid loading everything to GPU memory if only one worker is used\n",
    "        clip = torch.load(path, map_location='cpu').float()  # (C, T, H, W) tensor\n",
    "        \n",
    "        return clip, label\n",
    "\n",
    "# === 3. Load pretrained I3D ===\n",
    "num_classes = 2 \n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", \"i3d_r50\", pretrained=True)\n",
    "\n",
    "# ✅ Replace last layer\n",
    "model.blocks[6].proj = nn.Linear(model.blocks[6].proj.in_features, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# === 4. Training setup (PATH/CLASS/DATASET CHANGES APPLIED) ===\n",
    "# ----------------------------------------------------------------------\n",
    "# 🔥 PATH CHANGE APPLIED HERE 🔥\n",
    "# Set the root directory\n",
    "data_root = \"D:\\I3D\" \n",
    "# Define the final model classes (Snatching=0, Normal=1)\n",
    "classes = [\"Snatching\", \"Normal\"] \n",
    "\n",
    "# 🔥 Change 3: Use the new PtClipDataset class and correct split names\n",
    "# Split names must be \"TRAINDATA\" and \"TESTDATA\" to match your disk structure\n",
    "train_ds = PtClipDataset(data_root, split=\"TRAINDATA\")\n",
    "val_ds = PtClipDataset(data_root, split=\"TESTDATA\")\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(f\"Loaded {len(train_ds)} training samples and {len(val_ds)} validation samples.\")\n",
    "if len(train_ds) == 0:\n",
    "    raise ValueError(\"Training dataset is empty. Check your D:\\\\I3D\\\\TRAINDATA path and file extensions (.pt).\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Early stopping\n",
    "patience = 10\n",
    "best_val_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "\n",
    "# === 5. Training loop ===\n",
    "for epoch in range(1, 51): \n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nEpoch {epoch}/50\")\n",
    "\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    train_losses, train_preds, train_targs = [], [], []\n",
    "    for clips, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        clips, labels = clips.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(clips)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_losses.append(loss.item())\n",
    "        train_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "        train_targs += labels.cpu().tolist()\n",
    "\n",
    "    # Train metrics\n",
    "    train_acc = accuracy_score(train_targs, train_preds)\n",
    "    train_prec, train_rec, train_f1, _ = precision_recall_fscore_support(\n",
    "        train_targs, train_preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_losses, val_preds, val_targs, val_probs = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for clips, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            clips, labels = clips.to(device), labels.to(device)\n",
    "            outputs = model(clips)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_losses.append(loss.item())\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            val_probs += probs.cpu().tolist()\n",
    "            val_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "            val_targs += labels.cpu().tolist()\n",
    "\n",
    "    # Val metrics (default argmax threshold)\n",
    "    val_acc = accuracy_score(val_targs, val_preds)\n",
    "    val_prec, val_rec, val_f1, _ = precision_recall_fscore_support(\n",
    "        val_targs, val_preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # Log epoch results\n",
    "    elapsed = time.time() - epoch_start\n",
    "    print(f\"Time: {elapsed:.1f}s\")\n",
    "    print(f\" Train Loss: {np.mean(train_losses):.4f} | Acc: {train_acc:.4f} | \"\n",
    "          f\"P: {train_prec:.4f} R: {train_rec:.4f} F1: {train_f1:.4f}\")\n",
    "    print(f\" Val   Loss: {np.mean(val_losses):.4f} | Acc: {val_acc:.4f} | \"\n",
    "          f\"P: {val_prec:.4f} R: {val_rec:.4f} F1: {val_f1:.4f}\")\n",
    "\n",
    "    # --- Save checkpoints ---\n",
    "    torch.save(model.state_dict(), \"last.pt\") \n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), \"aug_data_i3d.pt\")\n",
    "        print(f\" 🔥 Saved new best model with F1={best_val_f1:.4f}\")\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f\" Patience counter: {no_improve_epochs}/{patience}\")\n",
    "    # Early stopping\n",
    "    if no_improve_epochs >= patience:\n",
    "        print(\"⏹ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# === 6. Confusion Matrix + Thresholding (post-training) ===\n",
    "print(\"\\n=== Final Evaluation with Thresholding ===\")\n",
    "model.eval()\n",
    "all_probs, all_preds, all_labels = [], [], []\n",
    "with torch.no_grad():\n",
    "    for clips, labels in tqdm(val_loader, desc=\"Final Eval\"):\n",
    "        clips, labels = clips.to(device), labels.to(device)\n",
    "        outputs = model(clips)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs += probs.cpu().tolist()\n",
    "        all_labels += labels.cpu().tolist()\n",
    "\n",
    "# Apply threshold tuning\n",
    "all_probs = np.array(all_probs)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Default argmax predictions\n",
    "default_preds = np.argmax(all_probs, axis=1)\n",
    "\n",
    "# Threshold tuning: Snatching = 0, Normal = 1\n",
    "threshold_preds = []\n",
    "for p in all_probs:\n",
    "    if p[0] > 0.7: \n",
    "        threshold_preds.append(0)\n",
    "    else:\n",
    "        threshold_preds.append(1) \n",
    "\n",
    "print(\"\\n--- Default Evaluation ---\")\n",
    "print(classification_report(all_labels, default_preds, target_names=classes))\n",
    "cm = confusion_matrix(all_labels, default_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Confusion Matrix - Default Argmax\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Thresholded Evaluation (Snatching > 0.7) ---\")\n",
    "print(classification_report(all_labels, threshold_preds, target_names=classes))\n",
    "cm_thr = confusion_matrix(all_labels, threshold_preds)\n",
    "sns.heatmap(cm_thr, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Confusion Matrix - Thresholded (0.7 for Snatching)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WH1JmO6CdqGY"
   },
   "source": [
    "**DAta Preparation for new update dataset 94%->96% accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Avijit/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1013 training samples and 127 validation samples.\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# === 1. Imports ===\n",
    "import os, random, time\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    ")\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === 2. Dataset (MODIFIED for .pt files and flexible class mapping) ===\n",
    "class PtClipDataset(Dataset):\n",
    "    # Mapping the folder names (keys) to the model class index (values)\n",
    "    # Snatching=0, Normal=1\n",
    "    CLASS_MAP = {\n",
    "        \"person\": 1,     # Assuming 'person' folder contains 'Normal' clips (index 1)\n",
    "        \"snatcher\": 0,   # Assuming 'snatcher' folder contains 'Snatching' clips (index 0)\n",
    "        \"victim\": 0      # Assuming 'victim' folder contains 'Snatching' clips (index 0)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, root, split=\"TRAINDATA\"): \n",
    "        self.samples = []\n",
    "        \n",
    "        # Get all relevant class folders inside the split directory\n",
    "        split_path = os.path.join(root, split)\n",
    "        if not os.path.isdir(split_path):\n",
    "            print(f\"Error: Split directory not found at {split_path}\")\n",
    "            return\n",
    "\n",
    "        # Iterate only over the folders we've defined a mapping for\n",
    "        for folder_name, class_index in self.CLASS_MAP.items():\n",
    "            folder = os.path.join(split_path, folder_name)\n",
    "            if not os.path.isdir(folder):\n",
    "                print(f\"Warning: Class folder not found at {folder}\")\n",
    "                continue\n",
    "            \n",
    "            # Searching for \"*.pt\" files\n",
    "            for f in glob(os.path.join(folder, \"*.pt\")):\n",
    "                self.samples.append((f, class_index))\n",
    "        \n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        \n",
    "        # Using torch.load() for .pt files\n",
    "        # Using map_location='cpu' to load data outside the main DataLoader worker thread\n",
    "        clip = torch.load(path, map_location='cpu').float()  # (C, T, H, W) tensor\n",
    "        \n",
    "        return clip, label\n",
    "\n",
    "# === 3. Load pretrained I3D ===\n",
    "num_classes = 2 \n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", \"i3d_r50\", pretrained=True)\n",
    "\n",
    "# ✅ Replace last layer\n",
    "model.blocks[6].proj = nn.Linear(model.blocks[6].proj.in_features, num_classes)\n",
    "\n",
    "# Set device and move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# === 4. Training setup (PATHS and DATASET ARE CORRECT) ===\n",
    "# ----------------------------------------------------------------------\n",
    "# Root directory is set to D:\\I3D\n",
    "data_root = \"D:\\I3D\" \n",
    "# Define the final model classes \n",
    "classes = [\"Snatching\", \"Normal\"] \n",
    "\n",
    "# Use the PtClipDataset with correct split names\n",
    "train_ds = PtClipDataset(data_root, split=\"TRAINDATA\")\n",
    "val_ds = PtClipDataset(data_root, split=\"TESTDATA\")\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(f\"Loaded {len(train_ds)} training samples and {len(val_ds)} validation samples.\")\n",
    "if len(train_ds) == 0:\n",
    "    raise ValueError(\"Training dataset is empty. Check your D:\\\\I3D\\\\TRAINDATA path and file extensions (.pt).\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Early stopping\n",
    "patience = 10\n",
    "best_val_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "\n",
    "# === 5. Training loop (Set up for GPU/CUDA with autocast and GradScaler) ===\n",
    "for epoch in range(1, 51): \n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nEpoch {epoch}/50\")\n",
    "\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    train_losses, train_preds, train_targs = [], [], []\n",
    "    for clips, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        clips, labels = clips.to(device), labels.to(device) # Data moved to GPU\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(clips)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_losses.append(loss.item())\n",
    "        train_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "        train_targs += labels.cpu().tolist()\n",
    "\n",
    "    # Train metrics\n",
    "    train_acc = accuracy_score(train_targs, train_preds)\n",
    "    train_prec, train_rec, train_f1, _ = precision_recall_fscore_support(\n",
    "        train_targs, train_preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_losses, val_preds, val_targs, val_probs = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for clips, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            clips, labels = clips.to(device), labels.to(device) # Data moved to GPU\n",
    "            outputs = model(clips)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_losses.append(loss.item())\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            val_probs += probs.cpu().tolist()\n",
    "            val_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "            val_targs += labels.cpu().tolist()\n",
    "\n",
    "    # Val metrics (default argmax threshold)\n",
    "    val_acc = accuracy_score(val_targs, val_preds)\n",
    "    val_prec, val_rec, val_f1, _ = precision_recall_fscore_support(\n",
    "        val_targs, val_preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # Log epoch results\n",
    "    elapsed = time.time() - epoch_start\n",
    "    print(f\"Time: {elapsed:.1f}s\")\n",
    "    print(f\" Train Loss: {np.mean(train_losses):.4f} | Acc: {train_acc:.4f} | \"\n",
    "          f\"P: {train_prec:.4f} R: {train_rec:.4f} F1: {train_f1:.4f}\")\n",
    "    print(f\" Val   Loss: {np.mean(val_losses):.4f} | Acc: {val_acc:.4f} | \"\n",
    "          f\"P: {val_prec:.4f} R: {val_rec:.4f} F1: {val_f1:.4f}\")\n",
    "\n",
    "    # --- Save checkpoints ---\n",
    "    torch.save(model.state_dict(), \"last.pt\") \n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), \"aug_data_i3d.pt\")\n",
    "        print(f\" 🔥 Saved new best model with F1={best_val_f1:.4f}\")\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f\" Patience counter: {no_improve_epochs}/{patience}\")\n",
    "    # Early stopping\n",
    "    if no_improve_epochs >= patience:\n",
    "        print(\"⏹ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# === 6. Confusion Matrix + Thresholding (post-training) ===\n",
    "print(\"\\n=== Final Evaluation with Thresholding ===\")\n",
    "model.eval()\n",
    "all_probs, all_preds, all_labels = [], [], []\n",
    "with torch.no_grad():\n",
    "    for clips, labels in tqdm(val_loader, desc=\"Final Eval\"):\n",
    "        clips, labels = clips.to(device), labels.to(device)\n",
    "        outputs = model(clips)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs += probs.cpu().tolist()\n",
    "        all_labels += labels.cpu().tolist()\n",
    "\n",
    "# Apply threshold tuning\n",
    "all_probs = np.array(all_probs)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Default argmax predictions\n",
    "default_preds = np.argmax(all_probs, axis=1)\n",
    "\n",
    "# Threshold tuning: Snatching = 0, Normal = 1\n",
    "threshold_preds = []\n",
    "for p in all_probs:\n",
    "    if p[0] > 0.7: \n",
    "        threshold_preds.append(0)\n",
    "    else:\n",
    "        threshold_preds.append(1) \n",
    "\n",
    "print(\"\\n--- Default Evaluation ---\")\n",
    "print(classification_report(all_labels, default_preds, target_names=classes))\n",
    "cm = confusion_matrix(all_labels, default_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Confusion Matrix - Default Argmax\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Thresholded Evaluation (Snatching > 0.7) ---\")\n",
    "print(classification_report(all_labels, threshold_preds, target_names=classes))\n",
    "cm_thr = confusion_matrix(all_labels, threshold_preds)\n",
    "sns.heatmap(cm_thr, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Confusion Matrix - Thresholded (0.7 for Snatching)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base I3D model architecture...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Avijit/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded weights from D:\\I3D\\SRC\\aug_data_i3d.pt\n",
      "Starting evaluation on 1513 validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 757/757 [03:12<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL MODEL PERFORMANCE ANALYSIS ===\n",
      "Total Evaluation Time: 192.42s\n",
      "Total Samples: 1513\n",
      "\n",
      "--- Classification Report (Per-Class Metrics) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Snatcher       0.00      0.00      0.00       297\n",
      "      Victim       0.00      0.00      0.00       425\n",
      "      Person       0.52      1.00      0.69       791\n",
      "\n",
      "    accuracy                           0.52      1513\n",
      "   macro avg       0.17      0.33      0.23      1513\n",
      "weighted avg       0.27      0.52      0.36      1513\n",
      "\n",
      "\n",
      "--- Confusion Matrix Data ---\n",
      "[[  0   0 297]\n",
      " [  0   0 425]\n",
      " [  0   0 791]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, random, time\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    ")\n",
    "# 🔥 FIX: Ensure tqdm is explicitly imported\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "# === 1. Essential Setup (Must match Training Script) ===\n",
    "\n",
    "# Define the model paths and configuration\n",
    "MODEL_PATH = \"D:\\\\I3D\\\\SRC\\\\aug_data_i3d.pt\"\n",
    "DATA_ROOT = \"D:\\\\I3D\"\n",
    "VAL_SPLIT = \"yashtidataset\"\n",
    "num_classes = 3\n",
    "classes = [\"Snatcher\", \"Victim\", \"Person\"] \n",
    "BATCH_SIZE = 2 # Must match the batch size used for stability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Re-define the Dataset class used during training\n",
    "class PtClipDataset(Dataset):\n",
    "    CLASS_MAP = {\n",
    "        \"snatcher\": 0, \"victim\": 1, \"person\": 2\n",
    "    }\n",
    "    def __init__(self, root, split=\"TRAINDATA\"): \n",
    "        self.samples = []\n",
    "        split_path = os.path.join(root, split)\n",
    "        if not os.path.isdir(split_path):\n",
    "            sys.stdout.write(f\"Error: Split directory not found at {split_path}\\n\"); return\n",
    "        for folder_name, class_index in self.CLASS_MAP.items():\n",
    "            folder = os.path.join(split_path, folder_name)\n",
    "            if os.path.isdir(folder):\n",
    "                for f in glob(os.path.join(folder, \"*.pt\")):\n",
    "                    self.samples.append((f, class_index))\n",
    "        random.shuffle(self.samples)\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        clip = torch.load(path, map_location='cpu').float()\n",
    "        if clip.shape[0] != 3:  # Fix dimension order if needed\n",
    "            clip = clip.permute(1, 0, 2, 3)\n",
    "        return clip, label\n",
    "\n",
    "\n",
    "# === 2. Model Loading and Weight Transfer ===\n",
    "\n",
    "# 1. Instantiate the I3D Model\n",
    "sys.stdout.write(\"Loading base I3D model architecture...\\n\")\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", \"i3d_r50\", pretrained=True)\n",
    "\n",
    "# 2. Re-apply the classification head change\n",
    "model.blocks[6].proj = nn.Linear(model.blocks[6].proj.in_features, num_classes)\n",
    "\n",
    "# 3. Load the best saved weights\n",
    "try:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    sys.stdout.write(f\"Successfully loaded weights from {MODEL_PATH}\\n\")\n",
    "except Exception as e:\n",
    "    sys.stdout.write(f\"Error loading model weights: {e}\\n\")\n",
    "    sys.exit()\n",
    "\n",
    "# 4. Move model to device and set to evaluation mode\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === 3. Data Loading ===\n",
    "val_ds = PtClipDataset(DATA_ROOT, split=VAL_SPLIT)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "sys.stdout.write(f\"Starting evaluation on {len(val_ds)} validation samples...\\n\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "# === 4. Evaluation Loop ===\n",
    "\n",
    "all_probs, all_preds, all_labels = [], [], []\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clips, labels in tqdm(val_loader, desc=\"Evaluation\"):\n",
    "        clips, labels = clips.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(clips)\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs += probs.cpu().tolist()\n",
    "        all_labels += labels.cpu().tolist()\n",
    "        all_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "\n",
    "# Convert to numpy for sklearn metrics\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# === 5. Metrics and Reporting ===\n",
    "\n",
    "sys.stdout.write(\"\\n=== FINAL MODEL PERFORMANCE ANALYSIS ===\\n\")\n",
    "sys.stdout.write(f\"Total Evaluation Time: {elapsed_time:.2f}s\\n\")\n",
    "sys.stdout.write(f\"Total Samples: {len(all_labels)}\\n\")\n",
    "\n",
    "# 1. Classification Report\n",
    "sys.stdout.write(\"\\n--- Classification Report (Per-Class Metrics) ---\\n\")\n",
    "report = classification_report(all_labels, all_preds, target_names=classes, zero_division=0)\n",
    "sys.stdout.write(report + \"\\n\")\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sys.stdout.write(\"\\n--- Confusion Matrix Data ---\\n\")\n",
    "sys.stdout.write(str(cm) + \"\\n\")\n",
    "\n",
    "# Plotting the Confusion Matrix \n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Confusion Matrix - aug_data_i3d.pt\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.show()\n",
    "\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\I3D\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-10-12 16:27:21,981] A new study created in memory with name: no-name-8d0bd221-26bd-4326-bb5d-fa152cf6fbe4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Optuna Hyperparameter Tuning for 10 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Using cache found in C:\\Users\\Avijit/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n",
      "  0%|          | 0/10 [02:28<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-10-12 16:29:50,528] Trial 0 failed with parameters: {'lr_head': 0.00014768562989120977, 'lr_base': 1.1155822782804363e-06, 'weight_decay': 0.00023520225499502727, 'patience': 11} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\I3D\\env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Avijit\\AppData\\Local\\Temp\\ipykernel_13060\\76703043.py\", line 113, in objective\n",
      "    clips, labels = clips.to(device), labels.to(device)\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-12 16:29:50,538] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 169\u001b[0m\n\u001b[0;32m    166\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAn error occurred during optimization: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\I3D\\env\\lib\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\I3D\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32md:\\I3D\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32md:\\I3D\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    257\u001b[0m ):\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[1;32md:\\I3D\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[6], line 113\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# --- Train ---\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (clips, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m--> 113\u001b[0m     clips, labels \u001b[38;5;241m=\u001b[39m \u001b[43mclips\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m    115\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(clips)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, random, time\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "import optuna # Import Optuna\n",
    "\n",
    "# === GLOBAL CONFIGURATION (FIXED PARAMETERS) ===\n",
    "MAX_EPOCHS = 4 \n",
    "BATCH_SIZE = 2 \n",
    "ACCUMULATION_STEPS = 4 \n",
    "N_TRIALS = 10 # Number of hyperparameter combinations (trials) to test\n",
    "\n",
    "# === 2. Dataset Definition (Simplified for Optuna structure) ===\n",
    "# (PtClipDataset, train_loader, val_loader, and class weights calculation are assumed to be outside the objective function for speed)\n",
    "\n",
    "class PtClipDataset(Dataset):\n",
    "    CLASS_MAP = {\"snatcher\": 0, \"victim\": 1, \"person\": 2}\n",
    "    def __init__(self, root, split=\"TRAINDATA\"): \n",
    "        self.samples = []\n",
    "        split_path = os.path.join(root, split)\n",
    "        if not os.path.isdir(split_path): sys.stdout.write(f\"Error: Split directory not found at {split_path}\\n\"); return\n",
    "        for folder_name, class_index in self.CLASS_MAP.items():\n",
    "            folder = os.path.join(split_path, folder_name)\n",
    "            if os.path.isdir(folder):\n",
    "                for f in glob(os.path.join(folder, \"*.pt\")): self.samples.append((f, class_index))\n",
    "        random.shuffle(self.samples)\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        clip = torch.load(path, map_location='cpu').float()\n",
    "        return clip, label\n",
    "    def get_labels(self): return [label for _, label in self.samples]\n",
    "\n",
    "# === 3. Model & Data Initialization (Setup outside the objective function) ===\n",
    "num_classes = 3 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_root = \"D:\\I3D\" \n",
    "classes = [\"Snatcher\", \"Victim\", \"Person\"] \n",
    "\n",
    "train_ds = PtClipDataset(data_root, split=\"TRAINDATA\")\n",
    "val_ds = PtClipDataset(data_root, split=\"TESTDATA\")\n",
    "\n",
    "if len(train_ds) == 0:\n",
    "    sys.stdout.write(\"Error: Training dataset is empty. Cannot run tuning.\\n\"); sys.exit()\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "train_labels = train_ds.get_labels()\n",
    "class_counts = np.bincount(train_labels, minlength=num_classes)\n",
    "total_samples = len(train_labels)\n",
    "class_weights = total_samples / class_counts\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "# === 5. OPTUNA OBJECTIVE FUNCTION ===\n",
    "\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    \"\"\"\n",
    "    Defines the training routine for one Optuna trial and samples hyperparameters.\n",
    "    The goal is to maximize the Validation F1 Score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Hyperparameter Sampling\n",
    "    lr_head = trial.suggest_float(\"lr_head\", 1e-5, 5e-4, log=True)\n",
    "    lr_base = trial.suggest_float(\"lr_base\", 1e-6, 5e-5, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 1e-3, log=True)\n",
    "    patience = trial.suggest_int(\"patience\", 5, 15)\n",
    "    \n",
    "    # 2. Model Initialization (needs to be fresh for every trial)\n",
    "    model = torch.hub.load(\"facebookresearch/pytorchvideo\", \"i3d_r50\", pretrained=True)\n",
    "    \n",
    "    # Freeze layers and replace head (as before)\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith('blocks.5') or name.startswith('blocks.6'):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    final_proj_layer = nn.Linear(model.blocks[6].proj.in_features, num_classes)\n",
    "    model.blocks[6].proj = final_proj_layer\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 3. Optimizer and Criterion Setup\n",
    "    optimizer = torch.optim.SGD([\n",
    "        {'params': [p for name, p in model.named_parameters() \n",
    "                    if p.requires_grad and 'proj' not in name], \n",
    "         'lr': lr_base},\n",
    "        {'params': final_proj_layer.parameters(), \n",
    "         'lr': lr_head}\n",
    "    ], momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1) \n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_val_f1 = -1.0\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    # 4. Training Loop\n",
    "    for epoch in range(1, MAX_EPOCHS + 1): \n",
    "        model.train()\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # --- Train ---\n",
    "        for batch_idx, (clips, labels) in enumerate(train_loader):\n",
    "            clips, labels = clips.to(device), labels.to(device) \n",
    "            with autocast():\n",
    "                outputs = model(clips)\n",
    "                loss = criterion(outputs, labels) / ACCUMULATION_STEPS \n",
    "            scaler.scale(loss).backward() \n",
    "            \n",
    "            if (batch_idx + 1) % ACCUMULATION_STEPS == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_preds, val_targs = [], []\n",
    "        with torch.no_grad():\n",
    "            for clips, labels in val_loader:\n",
    "                clips, labels = clips.to(device), labels.to(device) \n",
    "                outputs = model(clips)\n",
    "                val_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "                val_targs += labels.cpu().tolist()\n",
    "\n",
    "        val_prec, val_rec, val_f1, _ = precision_recall_fscore_support(\n",
    "            val_targs, val_preds, average=\"weighted\", zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Update best F1 and check patience\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            \n",
    "        if no_improve_epochs >= patience:\n",
    "            break\n",
    "            \n",
    "        # 5. Report intermediate result to Optuna (pruning)\n",
    "        trial.report(val_f1, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_val_f1\n",
    "\n",
    "# === 6. OPTUNA EXECUTION ===\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 1. Create a study object and optimize the objective function\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    \n",
    "    sys.stdout.write(f\"\\nStarting Optuna Hyperparameter Tuning for {N_TRIALS} trials...\\n\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "    except Exception as e:\n",
    "        sys.stdout.write(f\"\\nAn error occurred during optimization: {e}\\n\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    # 2. Final Summary\n",
    "    sys.stdout.write(\"\\n============================================\\n\")\n",
    "    sys.stdout.write(f\"🏆 BEST OPTUNA RESULT (Val F1={study.best_value:.4f}):\\n\")\n",
    "    sys.stdout.write(\"============================================\\n\")\n",
    "    for key, value in study.best_params.items():\n",
    "        sys.stdout.write(f\"  {key}: {value:.2e}\\n\")\n",
    "\n",
    "    # Optional: Save the best model configuration file\n",
    "    best_lr_h = study.best_params['lr_head']\n",
    "    best_lr_b = study.best_params['lr_base']\n",
    "    best_wd = study.best_params['weight_decay']\n",
    "\n",
    "    sys.stdout.write(f\"\\nFinal best model weights saved at:\\n\")\n",
    "    sys.stdout.write(f\"D:\\\\I3D\\\\SRC\\\\optuna_best_lr{best_lr_h:.0e}_wd{best_wd:.0e}.pt\\n\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5mmOy5bd9oZ"
   },
   "outputs": [],
   "source": [
    "import os, cv2, numpy as np, random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "# ✅ Final Settings\n",
    "input_root = \"/content/drive/MyDrive/Dataset\"\n",
    "output_root = \"/content/clips_dataset_32frames_double_final\"\n",
    "classes = [\"Snatching\", \"Normal\"]\n",
    "\n",
    "num_frames = 32   # frames per clip\n",
    "val_split = 0.2   # % for validation\n",
    "k = 2             # ✅ how many clips per video (1=original, >1=multiple augmentations)\n",
    "\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "def simplified_sampling(total_frames, num_samples=32):\n",
    "    return np.linspace(0, max(0, total_frames-1), num_samples, dtype=int).tolist()\n",
    "\n",
    "def apply_augmentation(frames):\n",
    "    \"\"\"Apply random augmentation on the whole video\"\"\"\n",
    "    augmented_frames = []\n",
    "    # Decide augmentation params once for all frames\n",
    "    do_flip = random.random() < 0.5\n",
    "    do_brightness = random.random() < 0.3\n",
    "    brightness_factor = random.uniform(0.7, 1.3)\n",
    "    do_contrast = random.random() < 0.25\n",
    "    contrast_factor = random.uniform(0.8, 1.2)\n",
    "    do_saturation = random.random() < 0.2\n",
    "    saturation_factor = random.uniform(0.8, 1.2)\n",
    "    do_rotation = random.random() < 0.15\n",
    "    rotation_angle = random.uniform(-8, 8)\n",
    "\n",
    "    for frame in frames:\n",
    "        pil_frame = Image.fromarray(frame)\n",
    "        if do_flip:\n",
    "            pil_frame = pil_frame.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        if do_brightness:\n",
    "            pil_frame = ImageEnhance.Brightness(pil_frame).enhance(brightness_factor)\n",
    "        if do_contrast:\n",
    "            pil_frame = ImageEnhance.Contrast(pil_frame).enhance(contrast_factor)\n",
    "        if do_saturation:\n",
    "            pil_frame = ImageEnhance.Color(pil_frame).enhance(saturation_factor)\n",
    "        if do_rotation:\n",
    "            pil_frame = pil_frame.rotate(rotation_angle, fillcolor=(128, 128, 128))\n",
    "        augmented_frames.append(np.array(pil_frame))\n",
    "    return augmented_frames\n",
    "\n",
    "# ✅ Main Processing Loop\n",
    "for cls in classes:\n",
    "    in_dir = os.path.join(input_root, cls)\n",
    "    videos = sorted(os.listdir(in_dir))\n",
    "    random.shuffle(videos)\n",
    "\n",
    "    split_idx = int(len(videos) * (1 - val_split))\n",
    "    train_videos = videos[:split_idx]\n",
    "    val_videos = videos[split_idx:]\n",
    "\n",
    "    for split, vids in ((\"train\", train_videos), (\"val\", val_videos)):\n",
    "        out_dir = os.path.join(output_root, split, cls)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        print(f\"Processing {len(vids)} videos for {split}/{cls} (k={k}, {num_frames} frames each)\")\n",
    "\n",
    "        for vid in tqdm(vids):\n",
    "            path = os.path.join(in_dir, vid)\n",
    "            cap = cv2.VideoCapture(path)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            indices = simplified_sampling(total_frames, num_frames)\n",
    "\n",
    "            frames = []\n",
    "            i = 0\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                if i in indices:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frame_resized = cv2.resize(frame_rgb, (224, 224))\n",
    "                    frames.append(frame_resized)\n",
    "                i += 1\n",
    "            cap.release()\n",
    "\n",
    "            if len(frames) == 0:\n",
    "                print(\"⚠️ Empty video:\", path)\n",
    "                continue\n",
    "\n",
    "            # Pad if too short\n",
    "            while len(frames) < num_frames:\n",
    "                frames.append(frames[-1].copy())\n",
    "\n",
    "            # === Generate k clips ===\n",
    "            for aug_id in range(1, k + 1):\n",
    "                if split == \"train\":\n",
    "                    clip_frames = apply_augmentation(frames)\n",
    "                else:\n",
    "                    clip_frames = frames  # no aug in val\n",
    "\n",
    "                # Normalize\n",
    "                clip = np.stack(clip_frames, axis=0).astype(np.float32) / 255.0\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                clip = (clip - mean) / std\n",
    "\n",
    "                # Convert to I3D format: (C, T, H, W)\n",
    "                clip = np.transpose(clip, (3, 0, 1, 2)).astype(np.float32)\n",
    "\n",
    "                # Save file\n",
    "                name = os.path.splitext(vid)[0]\n",
    "                out_name = f\"{name}_aug{aug_id}.npy\" if (k > 1 and split == \"train\") else f\"{name}.npy\"\n",
    "                np.save(os.path.join(out_dir, out_name), clip)\n",
    "\n",
    "print(\"✅ MULTI-K PIPELINE COMPLETE!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTszcFLDeGBp"
   },
   "source": [
    "**Model training base model as a 94% i3d**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKzOwLUseNta"
   },
   "outputs": [],
   "source": [
    "# === 1. Imports ===\n",
    "import os, random, time\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    ")\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === 2. Dataset ===\n",
    "class NpyClipDataset(Dataset):\n",
    "    def __init__(self, root, split=\"train\", classes=None):\n",
    "        self.samples = []\n",
    "        self.classes = classes or sorted(os.listdir(os.path.join(root, split)))\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            folder = os.path.join(root, split, cls)\n",
    "            if not os.path.isdir(folder):\n",
    "                continue\n",
    "            for f in glob(os.path.join(folder, \"*.npy\")):\n",
    "                self.samples.append((f, i))\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        clip = np.load(path).astype(np.float32)  # (C, T, H, W)\n",
    "        return torch.from_numpy(clip), label\n",
    "\n",
    "# === 3. Load pretrained I3D ===\n",
    "num_classes = 2  # Snatching / Normal\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", \"i3d_r50\", pretrained=True)\n",
    "\n",
    "# ✅ Replace last layer\n",
    "model.blocks[6].proj = nn.Linear(model.blocks[6].proj.in_features, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# === 4. Load best model weights ===\n",
    "best_model_path = \"/kaggle/input/best_secene_detection_model/pytorch/default/1/aug_data_i3d.pt\"   # <-- change to your best model file\n",
    "print(f\"Loading best model from {best_model_path} ...\")\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "# === 5. New dataset with augmentations + hard negatives ===\n",
    "data_root = \"/kaggle/input/dataset-k-i3d\"  # ✅ new dataset folder\n",
    "classes = [\"Snatching\", \"Normal\"]\n",
    "\n",
    "train_ds = NpyClipDataset(data_root, split=\"train\", classes=classes)\n",
    "val_ds = NpyClipDataset(data_root, split=\"val\", classes=classes)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-4, momentum=0.9, weight_decay=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Early stopping setup\n",
    "patience = 7\n",
    "best_val_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "\n",
    "# === 6. Fine-tuning loop (short training) ===\n",
    "for epoch in range(1, 21):  # only 10 epochs\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nEpoch {epoch}/10\")\n",
    "\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    train_losses, train_preds, train_targs = [], [], []\n",
    "    for clips, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        clips, labels = clips.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(clips)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_losses.append(loss.item())\n",
    "        train_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "        train_targs += labels.cpu().tolist()\n",
    "\n",
    "    # Train metrics\n",
    "    train_acc = accuracy_score(train_targs, train_preds)\n",
    "    train_prec, train_rec, train_f1, _ = precision_recall_fscore_support(\n",
    "        train_targs, train_preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_losses, val_preds, val_targs, val_probs = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for clips, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            clips, labels = clips.to(device), labels.to(device)\n",
    "            outputs = model(clips)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_losses.append(loss.item())\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            val_probs += probs.cpu().tolist()\n",
    "            val_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "            val_targs += labels.cpu().tolist()\n",
    "\n",
    "    # Val metrics\n",
    "    val_acc = accuracy_score(val_targs, val_preds)\n",
    "    val_prec, val_rec, val_f1, _ = precision_recall_fscore_support(\n",
    "        val_targs, val_preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - epoch_start\n",
    "    print(f\"Time: {elapsed:.1f}s\")\n",
    "    print(f\" Train Loss: {np.mean(train_losses):.4f} | Acc: {train_acc:.4f} | \"\n",
    "          f\"P: {train_prec:.4f} R: {train_rec:.4f} F1: {train_f1:.4f}\")\n",
    "    print(f\" Val   Loss: {np.mean(val_losses):.4f} | Acc: {val_acc:.4f} | \"\n",
    "          f\"P: {val_prec:.4f} R: {val_rec:.4f} F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save best\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), \"finetuned_aug_i3d_2.pt\")\n",
    "        print(f\"🔥 Saved improved model with F1={best_val_f1:.4f}\")\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f\" Patience counter: {no_improve_epochs}/{patience}\")\n",
    "    if no_improve_epochs >= patience:\n",
    "        print(\"⏹ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# === 7. Confusion Matrix + Threshold tuning ===\n",
    "print(\"\\n=== Final Evaluation with Thresholding ===\")\n",
    "model.eval()\n",
    "all_probs, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for clips, labels in tqdm(val_loader, desc=\"Final Eval\"):\n",
    "        clips, labels = clips.to(device), labels.to(device)\n",
    "        outputs = model(clips)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs += probs.cpu().tolist()\n",
    "        all_labels += labels.cpu().tolist()\n",
    "\n",
    "all_probs = np.array(all_probs)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Default\n",
    "default_preds = np.argmax(all_probs, axis=1)\n",
    "\n",
    "# Thresholded: Snatching > 0.7 else Normal\n",
    "threshold_preds = [0 if p[0] > 0.7 else 1 for p in all_probs]\n",
    "\n",
    "print(\"\\n--- Default Evaluation ---\")\n",
    "print(classification_report(all_labels, default_preds, target_names=classes))\n",
    "cm = confusion_matrix(all_labels, default_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Confusion Matrix - Default Argmax\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Thresholded Evaluation (Snatching > 0.7) ---\")\n",
    "print(classification_report(all_labels, threshold_preds, target_names=classes))\n",
    "cm_thr = confusion_matrix(all_labels, threshold_preds)\n",
    "sns.heatmap(cm_thr, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Confusion Matrix - Thresholded (0.7 for Snatching)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gibZ5GeqeiS4"
   },
   "source": [
    "upper without weight this one with weighth for **class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mOffIPnetSQ"
   },
   "outputs": [],
   "source": [
    "# === 1. Imports ===\n",
    "import os, random, time\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    ")\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === 2. Dataset ===\n",
    "class NpyClipDataset(Dataset):\n",
    "    def __init__(self, root, split=\"train\", classes=None):\n",
    "        self.samples = []\n",
    "        self.classes = classes or sorted(os.listdir(os.path.join(root, split)))\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            folder = os.path.join(root, split, cls)\n",
    "            if not os.path.isdir(folder):\n",
    "                continue\n",
    "            for f in glob(os.path.join(folder, \"*.npy\")):\n",
    "                self.samples.append((f, i))\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        clip = np.load(path).astype(np.float32)  # (C, T, H, W)\n",
    "        return torch.from_numpy(clip), label\n",
    "\n",
    "# === 3. Load pretrained I3D ===\n",
    "num_classes = 2  # Snatching / Normal\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", \"i3d_r50\", pretrained=True)\n",
    "\n",
    "# ✅ Replace last layer\n",
    "model.blocks[6].proj = nn.Linear(model.blocks[6].proj.in_features, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# === 4. Load best model weights ===\n",
    "best_model_path = \"/kaggle/input/best_secene_detection_model/pytorch/default/1/aug_data_i3d.pt\"   # <-- change to your best model file\n",
    "print(f\"Loading best model from {best_model_path} ...\")\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "# === 5. New dataset with augmentations + hard negatives ===\n",
    "data_root = \"/kaggle/input/dataset-k-i3d\"  # ✅ new dataset folder\n",
    "classes = [\"Snatching\", \"Normal\"]\n",
    "\n",
    "train_ds = NpyClipDataset(data_root, split=\"train\", classes=classes)\n",
    "val_ds = NpyClipDataset(data_root, split=\"val\", classes=classes)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# More weight for Normal (class 1)\n",
    "class_weights = torch.tensor([1.0, 1.3], dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-4, momentum=0.9, weight_decay=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Early stopping setup\n",
    "patience = 7\n",
    "best_val_f1 = 0.0\n",
    "best_val_loss = float(\"inf\")\n",
    "no_improve_epochs = 0\n",
    "\n",
    "# === 6. Fine-tuning loop (short training) ===\n",
    "for epoch in range(1, 21):  # only 10 epochs\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nEpoch {epoch}/10\")\n",
    "\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    train_losses, train_preds, train_targs = [], [], []\n",
    "    for clips, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        clips, labels = clips.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(clips)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_losses.append(loss.item())\n",
    "        train_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "        train_targs += labels.cpu().tolist()\n",
    "\n",
    "    # Train metrics\n",
    "    train_acc = accuracy_score(train_targs, train_preds)\n",
    "    train_prec, train_rec, train_f1, _ = precision_recall_fscore_support(\n",
    "        train_targs, train_preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_losses, val_preds, val_targs, val_probs = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for clips, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            clips, labels = clips.to(device), labels.to(device)\n",
    "            outputs = model(clips)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_losses.append(loss.item())\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            val_probs += probs.cpu().tolist()\n",
    "            val_preds += outputs.argmax(dim=1).cpu().tolist()\n",
    "            val_targs += labels.cpu().tolist()\n",
    "\n",
    "    # Val metrics\n",
    "    val_acc = accuracy_score(val_targs, val_preds)\n",
    "    val_prec, val_rec, val_f1, _ = precision_recall_fscore_support(\n",
    "        val_targs, val_preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - epoch_start\n",
    "    print(f\"Time: {elapsed:.1f}s\")\n",
    "    print(f\" Train Loss: {np.mean(train_losses):.4f} | Acc: {train_acc:.4f} | \"\n",
    "          f\"P: {train_prec:.4f} R: {train_rec:.4f} F1: {train_f1:.4f}\")\n",
    "    print(f\" Val   Loss: {np.mean(val_losses):.4f} | Acc: {val_acc:.4f} | \"\n",
    "          f\"P: {val_prec:.4f} R: {val_rec:.4f} F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save best\n",
    "    if (val_f1 > best_val_f1) or (val_f1 == best_val_f1 and np.mean(val_losses) < best_val_loss):\n",
    "        best_val_f1 = val_f1\n",
    "        best_val_loss = np.mean(val_losses)\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), \"finetuned_aug_i3d_final.pt\")\n",
    "        print(f\"🔥 Saved improved model with F1={best_val_f1:.4f}, Loss={best_val_loss:.4f}\")\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f\" Patience counter: {no_improve_epochs}/{patience}\")\n",
    "    if no_improve_epochs >= patience:\n",
    "        print(\"⏹ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# === 7. Confusion Matrix + Threshold tuning ===\n",
    "print(\"\\n=== Final Evaluation with Thresholding ===\")\n",
    "model.eval()\n",
    "all_probs, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for clips, labels in tqdm(val_loader, desc=\"Final Eval\"):\n",
    "        clips, labels = clips.to(device), labels.to(device)\n",
    "        outputs = model(clips)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs += probs.cpu().tolist()\n",
    "        all_labels += labels.cpu().tolist()\n",
    "\n",
    "all_probs = np.array(all_probs)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Default\n",
    "default_preds = np.argmax(all_probs, axis=1)\n",
    "\n",
    "# Thresholded: Snatching > 0.7 else Normal\n",
    "threshold_preds = [0 if p[0] > 0.7 else 1 for p in all_probs]\n",
    "\n",
    "print(\"\\n--- Default Evaluation ---\")\n",
    "print(classification_report(all_labels, default_preds, target_names=classes))\n",
    "cm = confusion_matrix(all_labels, default_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Confusion Matrix - Default Argmax\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Thresholded Evaluation (Snatching > 0.7) ---\")\n",
    "print(classification_report(all_labels, threshold_preds, target_names=classes))\n",
    "cm_thr = confusion_matrix(all_labels, threshold_preds)\n",
    "sns.heatmap(cm_thr, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Confusion Matrix - Thresholded (0.7 for Snatching)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMfGe116UqCTBjZwHhcGvOq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
